{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"/home/benjamin/RewardCurriculum/\")\n",
    "\n",
    "import os\n",
    "os.system(\"export MKL_SERVICE_FORCE_INTEL=1\")\n",
    "\n",
    "import gymnasium as gym\n",
    "from collections import defaultdict\n",
    "import panda_gym\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "import json\n",
    "import numpy as np\n",
    "import tqdm\n",
    "\n",
    "import os\n",
    "from utils.configs import get_config\n",
    "from utils.env_wrappers import make_vec_env, get_env\n",
    "from argparse import Namespace\n",
    "import cv2\n",
    "from IPython.display import HTML\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import LinearSegmentedColormap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as font_manager\n",
    "\n",
    "font_dir = ['.']\n",
    "for font in font_manager.findSystemFonts(font_dir):\n",
    "    font_manager.fontManager.addfont(font)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rollout_episodes(env, learner, n_episodes=1, max_steps=200, reset_every=20, task=None):\n",
    "    if task is None:\n",
    "        weights = np.zeros((1, learner.scheduler.reward_dim))\n",
    "        weights[:, -1] = 1\n",
    "\n",
    "    goals = []\n",
    "    rewards = []\n",
    "    \n",
    "    obs = env.reset()\n",
    "    current_goal = env.get_attr(\"task\")[0].get_goal()\n",
    "    goals.append(current_goal[3:])\n",
    "    n_steps = 0\n",
    "    \n",
    "    # if desired_position is not None:\n",
    "    #     position[3:] = desired_position\n",
    "    #     env.env_method(\"set_goal\", position)\n",
    "    #     obs[\"desired_goal\"] = position\n",
    "    \n",
    "    # if desired_position is not None:\n",
    "    #     positions.append(desired_position)\n",
    "    # else:\n",
    "    #     positions.append(obs[\"desired_goal\"][:, 3:])\n",
    "        \n",
    "    for step_idx in range(max_steps):            \n",
    "        act = learner.predict(obs, weights=weights, deterministic=False)[0]\n",
    "        obs, reward, _, info = env.step(act)\n",
    "        \n",
    "        if step_idx % reset_every == 0:\n",
    "            env.get_attr(\"task\")[0].reset()\n",
    "            current_goal = env.get_attr(\"task\")[0].get_goal()\n",
    "        \n",
    "        goals.append(current_goal[3:])\n",
    "        n_steps += 1\n",
    "\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    return rewards, goals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.chdir(\"/home/benjamin/RewardCurriculum\")\n",
    "results_dir = \"/home/benjamin/RewardCurriculum/results/panda_pick_and_place_obstacle_long_v2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generalization_results = defaultdict(list)\n",
    "\n",
    "master_goals = []\n",
    "master_rewards = []\n",
    "for subfolder in tqdm.tqdm(os.listdir(results_dir)):\n",
    "    folder_path = os.path.join(results_dir, subfolder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    \n",
    "    args = Namespace()\n",
    "    args.env_name = json.load(open(os.path.join(folder_path, \"config.json\"), \"r\"))[\"environment\"][\"env_name\"]\n",
    "    args.seed = json.load(open(os.path.join(folder_path, \"config.json\"), \"r\"))[\"seed\"]\n",
    "    args.config_path = os.path.join(folder_path, \"config_original.json\")\n",
    "    args.continue_from = None\n",
    "\n",
    "    remaining_args = [\"--environment.wrapper_kwargs.0.reward_threshold\", \"-0.05\"]\n",
    "    \n",
    "    config = get_config(args.config_path, args, remaining_args)\n",
    "    config[\"environment\"][\"wrappers\"] += [\"SingleTaskRewardWrapper\"]\n",
    "    config[\"environment\"][\"wrapper_kwargs\"] += [{}]\n",
    "    \n",
    "    make_env_fn = lambda wrappers, wrapper_kwargs, ignore_keyword=\"ignore\" : get_env(config[\"environment\"][\"env_name\"], wrappers=wrappers, wrapper_kwargs=wrapper_kwargs, ignore_keyword=ignore_keyword)\n",
    "    env = make_vec_env(make_env_fn, \n",
    "                        n_envs=config[\"environment\"][\"n_envs\"], \n",
    "                        env_kwargs={\"wrappers\": config[\"environment\"][\"wrappers\"], \"wrapper_kwargs\": config[\"environment\"][\"wrapper_kwargs\"]},\n",
    "                        monitor_kwargs={\"allow_early_resets\": True},\n",
    "                        seed=config[\"seed\"], vec_env_cls=DummyVecEnv)\n",
    "    \n",
    "    learner = config[\"learner_class\"].load(os.path.join(folder_path, \"evaluations\", \"best_model.zip\"), env=env)\n",
    "    \n",
    "    cl_type = str(config[\"learner_kwargs\"][\"scheduler_class\"]).split(\" \")[-1][2:-2]\n",
    "    # for position in tqdm.tqdm(desired_positions):\n",
    "    rewards, _ = rollout_episodes(env, learner, n_episodes=1, max_steps=1000, reset_every=10)\n",
    "    rewards = np.stack(rewards)\n",
    "    # goals = np.stack(goals)\n",
    "    \n",
    "    generalization_results[cl_type].append(rewards)\n",
    "    \n",
    "for cl_type in generalization_results.keys():\n",
    "    generalization_results[cl_type] = np.stack(generalization_results[cl_type])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'generalization_results' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mgeneralization_results\u001b[49m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSetterSolver\u001b[39m\u001b[38;5;124m\"\u001b[39m][:, np\u001b[38;5;241m.\u001b[39marange(\u001b[38;5;241m9\u001b[39m, \u001b[38;5;241m1009\u001b[39m, \u001b[38;5;241m10\u001b[39m), \u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(results\u001b[38;5;241m.\u001b[39msum(axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mmean())\n\u001b[1;32m      4\u001b[0m fig, ax \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m20\u001b[39m, \u001b[38;5;241m10\u001b[39m))\n",
      "\u001b[0;31mNameError\u001b[0m: name 'generalization_results' is not defined"
     ]
    }
   ],
   "source": [
    "results = generalization_results[\"SetterSolver\"][:, np.arange(9, 1009, 10), 0]\n",
    "print(results.sum(axis=1).mean())\n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(20, 10))\n",
    "\n",
    "plt.imshow(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(generalization_results, open(os.path.join(results_dir, \"generalization_perturbation.pkl\"), \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate a Latex table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "result_dirs = [\n",
    "    \"/home/benjamin/RewardCurriculum/results/panda_pick_and_place_sphere_long\",\n",
    "    \"/home/benjamin/RewardCurriculum/results/panda_pick_and_place_long\",\n",
    "    \"/home/benjamin/RewardCurriculum/results/panda_pick_and_place_obstacle_long_v2\"\n",
    "]\n",
    "\n",
    "generalization_results = [pickle.load(open(os.path.join(path, \"generalization_perturbation.pkl\"), \"rb\")) for path in result_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "$5.93$\\%\n",
      "$11.77$\\%\n",
      "$23.70$\\%\n",
      "$0.00$\\%\n",
      "$24.23$\\%\n",
      "$30.20$\\%\n",
      "$24.27$\\%\n"
     ]
    }
   ],
   "source": [
    "is_success_indices = np.arange(9, 1009, 10)\n",
    "# print(is_success_indices)\n",
    "\n",
    "algorithms = [\"ManualTask\", \"Random\", \"SACX\", \"BiPaRS\", \"SetterSolver\", \"CurrOT\", \"ALPGMM\"]\n",
    "\n",
    "for cl_type in algorithms:\n",
    "    \n",
    "    mean_results = []\n",
    "    std_results = []\n",
    "    processed_results = []\n",
    "    for idx, _generalization_results in enumerate(generalization_results):\n",
    "        # print(cl_type)\n",
    "        _results = _generalization_results.get(cl_type, np.zeros_like(_generalization_results[\"SetterSolver\"]))[:, is_success_indices, 0]\n",
    "        processed_results.append(_results)\n",
    "        mean_results.append(_results.sum(axis=1).mean())\n",
    "        std_results.append(_results.sum(axis=1).std())\n",
    "        # print(f\"${mean_results[idx]:.2f}$\\\\% \\\\par $\\\\pm {std_results[idx]:.2f}$ &\", end=\" \")\n",
    "        \n",
    "    mean_results = np.mean(mean_results)\n",
    "    processed_results = np.concatenate(processed_results, axis=0).sum(axis=0)\n",
    "    print(f\"${mean_results:.2f}$\\\\%\", end=\"\")\n",
    "         \n",
    "    print()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rew_curr",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
