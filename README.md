# Curriculum Learning Algorithms for Reward Weighting in a Sparse Reward Robotic Manipulation Tasks

Benjamin Fele, Jan Babiƒç<br>
Jozef Stefan Institute, Jamova cesta 39, 1000 Ljubljana, Slovenia <br>

[Paper](PaperLink)

## Abstract

>Robotic learning from sparse rewards can be a considerable challenge due to large amounts of data required for mastering a task. We explore the application of curriculum learning (CL) algorithms for automatic reward weighting within the context of multi-objective reinforcement learning (MORL) to tackle learning from sparse rewards in a robotic pick-and-place task. We take several state-of-the-art CL algorithms that were originally designed to generate curriculum by manipulating the environment, and appropriate them to weigh multiple sparse reward functions instead. The reward functions are chosen in a way that facilitates staged learning of the task, and the pick-and-place and stacking tasks is designed so that the agent learns to generalize to any initial and goal object position in the scene.
The results of our three implemented CL algorithms show large improvement over the naive and state-of-the-art baselines in terms of speed of convergence to a successful policy in a pick-and-place and stacking tasks with multiple task variations. Inspection of changes in reward weight values during training further reveals varying curricula generated by the employed approaches, and showcases shifting emphasis from auxiliary to the main reward as the training progresses.

## How to run

Clone the repository to your computer and install the dependencies listed in the `environment.yml` file. Note that the `panda-gym` library needs to be installed seperately by following the instructions in the `panda-gym/README.md`. You will also need to install the `nadaraya-watson` library to use CurrOT. Find the installation instructions in `external/nadaraya-watson`.

For running all the experiments from the paper, simply run:
```bash
python train_all.py
```

Please find the exact configurations in the `configs` folder. In `configs/templates/experiment.json`, you can also change the logging directory.

Individual experiments can be run by executing `train.py`. See the example below:
```bash
python train.py --config_path configs/experiment_tqc_setter_solver --env_name PandaMultiRewardPickAndPlaceSphereDense-v3
```

This will run one instance of the experiment using the Setter-Solver algorithm and learning to pick-and-place a sphere. (Please note that even though the environment name has the word "Dense" in it, wrappers take care of sparsifying the rewards.)